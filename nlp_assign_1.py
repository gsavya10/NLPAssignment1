""" AUTHORS 
Savyasachi Gupta (SXG220053)
Manav K Jain (MKJ210065)
"""

# -*- coding: utf-8 -*-
"""NLP Assign 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hjh_oqGkPubOoTP98Nc6sCdRURZ-IhB9
"""

# from google.colab import drive
# drive.mount('/content/drive/', force_remount=True)

# All project imports
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, WhitespaceTokenizer
from decimal import *
import math
import numpy as np

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# All project files
path = './A1_DATASET/'
file_name = "train.txt"
file_name_stop = 'train_stop.txt'
file_name_lemma = 'train_lemma.txt'
file_name_end = 'train_end.txt'
file_name_unk = 'train_unk.txt'

file_name_val = "val.txt"
file_name_stop_val = 'val_stop.txt'
file_name_lemma_val = 'val_lemma.txt'
file_name_end_val = 'val_end.txt'

TOKEN_UNK = '<UNK>'

"""# Preprocessing"""

# Removing Stop Words
def removeStopWords(file_name, file_name_stop):
    train_set = open(path + file_name, 'r')
    train_set_stop = open(path + file_name_stop, 'w')

    while True:
        line = train_set.readline()
        if not line:
            break

        tokens = word_tokenize(line.lower())
        english_stopwords = stopwords.words('english')
        tokens_wo_stopwords = [t for t in tokens if t not in english_stopwords and t.isalpha()]
        train_set_stop.write(" ".join(tokens_wo_stopwords)+"\n")

    train_set.close()
    train_set_stop.close()

# Lemmatization
def get_wordnet_pos(word):
    # Map POS tag to first character lemmatize() accepts
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)


def lemmatization(file_name_stop, file_name_lemma):
    train_set_stop = open(path + file_name_stop, 'r')
    train_set_lemma = open(path + file_name_lemma, 'w')

    lemmatizer = WordNetLemmatizer()

    while True:
        # Get next line from file
        line = train_set_stop.readline()
        if not line:
            break

        words = word_tokenize(line.lower())
        lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]
        train_set_lemma.write(" ".join(lemmatized_words)+"\n")

    train_set_stop.close()
    train_set_lemma.close()

# Adding [END] tags to all sentences
def addEndTags(file_name_lemma, file_name_end):
    train_set_lemma = open(path + file_name_lemma, 'r')
    train_set_end = open(path + file_name_end, 'w')

    while True:
        # Get next line from file
        line = train_set_lemma.readline()
        if not line:
            break

        train_set_end.write(line.strip()+" [END]\n")

    train_set_lemma.close()
    train_set_end.close()

def preprocessDataset(file_name, file_name_stop, file_name_lemma, file_name_end):
    removeStopWords(file_name, file_name_stop)
    lemmatization(file_name_stop, file_name_lemma)
    addEndTags(file_name_lemma, file_name_end)

# Preprocess Train dataset
preprocessDataset(file_name, file_name_stop, file_name_lemma, file_name_end)

# Preprocess Val dataset
preprocessDataset(file_name_val, file_name_stop_val, file_name_lemma_val, file_name_end_val)

"""# N-gram Modeling"""

# N-gram probabilities without smoothing
def nosmoothing(unigrams, bigrams, ngramCounts, total_word_count):
    unigramProbabilities = {}
    for unigram in unigrams:
        unigramProbabilities[unigram] = ngramCounts.get(unigram)/total_word_count

    bigramProbabilities = {}
    for bigram in bigrams:
        word1 = bigram[0]
        word2 = bigram[1]
        bigramProbabilities[bigram] = ngramCounts.get(bigram)/ngramCounts.get(word1)

    return unigramProbabilities, bigramProbabilities

# N-gram probabilities with Laplace smoothing
def smoothing_laplace(unigrams, bigrams, ngramCounts, total_word_count):
    unigramProbabilities = {}
    for unigram in unigrams:
        unigramProbabilities[unigram] = (ngramCounts.get(unigram) + 1) / (total_word_count + len(unigrams))

    bigramProbabilities = {}
    for bigram in bigrams:
        word1 = bigram[0]
        word2 = bigram[1]

        numerator = ngramCounts.get(bigram) + 1
        denominator = ngramCounts.get(word1) + len(unigrams)

        bigramProbabilities[bigram] = numerator / denominator

    return unigramProbabilities, bigramProbabilities

# N-gram probabilities with K smoothing
def smoothing_addK(unigrams, bigrams, ngramCounts, total_word_count, k):
    unigramProbabilities = {}
    for unigram in unigrams:
        unigramProbabilities[unigram] = (ngramCounts.get(unigram) + k) / (total_word_count + k*len(unigrams))

    bigramProbabilities = {}
    for bigram in bigrams:
        word1 = bigram[0]
        word2 = bigram[1]

        numerator = ngramCounts.get(bigram) + k
        denominator = ngramCounts.get(word1) + k*len(unigrams)

        bigramProbabilities[bigram] = numerator / denominator

    return unigramProbabilities, bigramProbabilities

def getNgrams(file_name, applyLaplaceSmoothing, applyKSmoothing, k = 0.01):
    file_content = open(path + file_name).read()

    wstk = WhitespaceTokenizer();
    tokens = wstk.tokenize(file_content)
    total_word_count = len(tokens)
    print(tokens)
    print(total_word_count)

    # Dictionary of n-gram counts
    ngramCounts = {}
    unigrams = []
    bigrams = []
    for i in range(total_word_count-1):

        # creating all bigrams
        bigram_curr = (tokens[i], tokens[i + 1])

        # bigram counts
        if bigram_curr in ngramCounts:
            ngramCounts[bigram_curr] += 1
        else:
            bigrams.append(bigram_curr)
            ngramCounts[bigram_curr] = 1

        # unigram counts
        if tokens[i] in ngramCounts:
            ngramCounts[tokens[i]] += 1
        else:
            unigrams.append(tokens[i])
            ngramCounts[tokens[i]] = 1

    # accounts for the one less iteration in loop for last token
    ngramCounts['[END]'] += 1
    bigram_curr = ('[END]', tokens[0])

    if bigram_curr in ngramCounts:
        ngramCounts[bigram_curr] += 1
    else:
        bigrams.append(bigram_curr)
        ngramCounts[bigram_curr] = 1

    unigramProbabilities = {}
    bigramProbabilities = {}

    if applyLaplaceSmoothing:
        unigramProbabilities, bigramProbabilities = smoothing_laplace(unigrams, bigrams, ngramCounts, total_word_count)
    elif applyKSmoothing:
        unigramProbabilities, bigramProbabilities = smoothing_addK(unigrams, bigrams, ngramCounts, total_word_count, k)
    else:
        unigramProbabilities, bigramProbabilities = nosmoothing(unigrams, bigrams, ngramCounts, total_word_count)

    return ngramCounts, unigrams, bigrams, unigramProbabilities, bigramProbabilities

def printNgrams(ngramCounts, unigrams, bigrams, unigramProbabilities, bigramProbabilities):
    print("unigrams list:-")
    print(unigrams)
    print(len(unigrams))

    print("\nbigrams list:-")
    print(bigrams)
    print(len(bigrams))

    print("\nUnigrams and their probabilities:-")
    print(unigramProbabilities)
    print(len(unigramProbabilities))

    print("\nBigrams and their probabilities:-")
    print(bigramProbabilities)
    print(len(bigramProbabilities))

ngramCounts, unigrams, bigrams, unigramProbabilities, bigramProbabilities = getNgrams(file_name_end, False, False)

printNgrams(ngramCounts, unigrams, bigrams, unigramProbabilities, bigramProbabilities)

"""# Unknown Words Handling"""

# Unknown word handling with top K words in vocabulary by frequency
k = 1000

keys = list(unigramProbabilities.keys())
values = list(unigramProbabilities.values())

sorted_value_index = np.flip(np.argsort(values))
topklist = sorted_value_index[0 : k]
topKdict = {keys[i]: values[i] for i in topklist}

print('Original vocabulary count: ', len(sorted_value_index))
print(topKdict)
print('New vocabulary count (top K): ', len(topKdict))

# New Train Set with Unknown Word in vocabulary
train_set_end = open(path + file_name_end, 'r')
train_set_unk = open(path + file_name_unk, 'w')

wstk = WhitespaceTokenizer();

while True:
    line = train_set_end.readline()
    if not line:
        break

    tokens = wstk.tokenize(line)

    # unigram counts
    for i in range(len(tokens)):
        if tokens[i] not in topKdict:
            tokens[i] = TOKEN_UNK

    train_set_unk.write(" ".join(tokens)+"\n")

train_set_end.close()
train_set_unk.close()

# New n-grams with unknown word handling
ngramCounts_unk, unigrams_unk, bigrams_unk, unigramProbabilities_unk, bigramProbabilities_unk = getNgrams(file_name_unk, False, False)
printNgrams(ngramCounts_unk, unigrams_unk, bigrams_unk, unigramProbabilities_unk, bigramProbabilities_unk)

"""# Smoothing"""

# New n-grams with unknown word handling & Laplace smoothing
ngramCounts_laplace, unigrams_laplace, bigrams_laplace, unigramProbabilities_laplace, bigramProbabilities_laplace = getNgrams(file_name_unk, True, False)
printNgrams(ngramCounts_laplace, unigrams_laplace, bigrams_laplace, unigramProbabilities_laplace, bigramProbabilities_laplace)

# New n-grams with unknown word handling & k smoothing & K = 0.05
K_smooth = 0.05
ngramCounts_k, unigrams_k, bigrams_k, unigramProbabilities_k, bigramProbabilities_k = getNgrams(file_name_unk, False, True, k=K_smooth)
printNgrams(ngramCounts_k, unigrams_k, bigrams_k, unigramProbabilities_k, bigramProbabilities_k)

"""# Perplexity"""

def getUnigramTokenProbability(token, unigramProbs):
    if token in unigramProbs:
        return unigramProbs[token]
    else:
        return unigramProbs[TOKEN_UNK]

def getBigramTokenProbability(token, ngramCounts, unigrams, bigramProbs, isLaplaceSmoothing = True, k = 0.01):
    word1 = token[0]
    word2 = token[1]

    if word1 not in unigrams:
        word1 = TOKEN_UNK
    if word2 not in unigrams:
        word2 = TOKEN_UNK

    token = (word1, word2)

    if token in bigramProbs:
        return bigramProbs[token]
    else:
        if isLaplaceSmoothing:
            return 1/(ngramCounts.get(word1) + len(unigrams))
        else:
            return k/(ngramCounts.get(word1) + k*len(unigrams))

def getUnigramProbOnValSet(wstk, line, unigramProbs):
    tokens = wstk.tokenize(line)
    totalProb = Decimal('1.0')

    for token in tokens:
        totalProb *= Decimal(getUnigramTokenProbability(token, unigramProbs))

    if totalProb == 0:
        print("ERROR: Zero probability occurred!!!")

    return totalProb, len(tokens)

def getBigramProbOnValSet(wstk, line, ngramCounts, unigrams, bigramProbs, isLaplaceSmoothing = True, k = 0.01):
    tokens = wstk.tokenize(line)
    totalProb = Decimal('1.0')

    token = ('[END]', tokens[0]) # start bigram token
    for i in range(len(tokens)):
        if i > 0:
            token = (tokens[i - 1], tokens[i])
        totalProb *= Decimal(getBigramTokenProbability(token, ngramCounts, unigrams, bigramProbs, isLaplaceSmoothing, k))

    if totalProb == 0:
        print("ERROR: Zero probability occurred!!!")

    return totalProb, len(tokens)

def getPerplexity(ngramCounts, unigrams, bigrams, unigramProbabilities,
                  bigramProbabilities, file_name_end_val, isUnigram,
                  applyLaplaceSmoothing = True, k = 0.01):
    val_set_end = open(path + file_name_end_val, 'r')
    wstk = WhitespaceTokenizer();

    totalWordCount = 0;
    p = Decimal('0')

    while True:
        line = val_set_end.readline()
        if not line:
            break

        lineProb = Decimal('1.0')
        lineWordCount = 0

        if isUnigram:
            lineProb, lineWordCount = getUnigramProbOnValSet(wstk, line, unigramProbabilities)
        else:
            if applyLaplaceSmoothing == True:
                lineProb, lineWordCount = getBigramProbOnValSet(wstk, line, ngramCounts, unigrams, bigramProbabilities, applyLaplaceSmoothing)
            else:
                lineProb, lineWordCount = getBigramProbOnValSet(wstk, line, ngramCounts, unigrams, bigramProbabilities, applyLaplaceSmoothing, k)

        totalWordCount += lineWordCount
        p += Decimal.ln(lineProb)/Decimal.ln(Decimal('2'))

    val_set_end.close()

    # Perplexity Calculations
    l = p / totalWordCount
    perplexity = 2**(-1*l)

    return perplexity

# Perplexity: Unigram, no smoothing
pp_unigram_unk = getPerplexity(ngramCounts_unk, unigrams_unk, bigrams_unk, unigramProbabilities_unk, bigramProbabilities_unk, file_name_end_val, True)
print("Perplexity: Unigram, no smoothing -> \t", pp_unigram_unk)

# Perplexity: Unigram, Laplace smoothing
pp_unigram_laplace = getPerplexity(ngramCounts_laplace, unigrams_laplace, bigrams_laplace, unigramProbabilities_laplace, bigramProbabilities_laplace, file_name_end_val, True)
print("Perplexity: Unigram, Laplace smoothing -> \t", pp_unigram_laplace)

# Perplexity: Unigram, K smoothing
pp_unigram_k = getPerplexity(ngramCounts_k, unigrams_k, bigrams_k, unigramProbabilities_k, bigramProbabilities_k, file_name_end_val, True)
print("Perplexity: Unigram, K smoothing -> \t", pp_unigram_k)

# Perplexity: Bigram, Laplace smoothing
pp_bigram_laplace = getPerplexity(ngramCounts_laplace, unigrams_laplace, bigrams_laplace, unigramProbabilities_laplace, bigramProbabilities_laplace, file_name_end_val, False)
print("Perplexity: Bigram, Laplace smoothing -> \t", pp_bigram_laplace)

# Perplexity: Bigram, K smoothing
pp_bigram_k = getPerplexity(ngramCounts_k, unigrams_k, bigrams_k, unigramProbabilities_k, bigramProbabilities_k, file_name_end_val, False, False, k=K_smooth)
print("Perplexity: Bigram, K smoothing -> \t", pp_bigram_k)

